{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOsEp5qyi3oT",
        "outputId": "1abe3745-0848-4db7-c989-cd59c410e24d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "from matplotlib.pyplot import *\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import roc_auc_score\n",
        "cuda = torch.cuda.is_available()\n",
        "cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD0TOfGEjHTu"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcxt1zqzjZhf"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSNXbq0jKg6"
      },
      "source": [
        "! kaggle datasets download -d yash612/covidnet-mini-and-gan-enerated-chest-xray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoHNBXnXkoKJ"
      },
      "source": [
        "! unzip covidnet-mini-and-gan-enerated-chest-xray.zip -d data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKCmMePGkwET"
      },
      "source": [
        "trans_apply=transforms.Compose([transforms.Resize((384,384)),transforms.ToTensor()])\n",
        "train_dataset=datasets.ImageFolder('data/chest_xray/chest_xray/train',transform=trans_apply)\n",
        "val_dataset=datasets.ImageFolder('data/chest_xray/chest_xray/val',transform=trans_apply)\n",
        "test_dataset=datasets.ImageFolder('data/chest_xray/chest_xray/test',transform=trans_apply)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tOD10MhmcZL"
      },
      "source": [
        "num_workers = 8 if cuda else 0 \n",
        "    \n",
        "# Training data\n",
        "train_loader_args = dict(shuffle=True, batch_size=8, num_workers=num_workers) if cuda\\\n",
        "                    else dict(shuffle=True, batch_size=16)\n",
        "train_loader = data.DataLoader(train_dataset, **train_loader_args)\n",
        "\n",
        "# Validation data\n",
        "val_loader_args = dict(shuffle=True, batch_size=8, num_workers=num_workers) if cuda\\\n",
        "                    else dict(shuffle=True, batch_size=16)\n",
        "val_loader = data.DataLoader(val_dataset, **val_loader_args)\n",
        "\n",
        "# Testing data\n",
        "test_loader_args = dict(shuffle=False, num_workers=num_workers) if cuda\\\n",
        "                    else dict(shuffle=False,drop_last=True)\n",
        "test_loader = data.DataLoader(test_dataset, **test_loader_args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzJiKi19lMkm"
      },
      "source": [
        "class Network_new(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Network_new, self).__init__()\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.layer4 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer5 = nn.Sequential(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer6 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.layer7 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer8 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer9 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer10 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.layer11 = nn.Sequential(nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer12 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer13 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer14 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.layer15 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer16 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer17 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        self.layer18 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        #self.layer1 = nn.Sequential(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.Dropout(0.3),)\n",
        "        \n",
        "        #self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        #self.classifier = nn.Sequential(nn.Linear(512*7*7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(),)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "        self.classifier = nn.Sequential(nn.Linear(512*4*4, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(),)\n",
        "        #nn.Linear(4096, num_classes),\n",
        "        self.linear_label_last = nn.Linear(4096, num_classes, bias=False)\n",
        "        # For creating the embedding to be passed into the Center Loss criterion\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.layer1(x)\n",
        "        output = self.layer2(output)\n",
        "        output = self.layer3(output)\n",
        "        output = self.layer4(output)\n",
        "        output = self.layer5(output)\n",
        "        output = self.layer6(output)\n",
        "        output = self.layer7(output)\n",
        "        output = self.layer8(output)\n",
        "        output = self.layer9(output)\n",
        "        output = self.layer10(output)\n",
        "        output = self.layer11(output)\n",
        "        output = self.layer12(output)\n",
        "        output = self.layer13(output)\n",
        "        output = self.layer14(output)\n",
        "        output = self.layer15(output)\n",
        "        output = self.layer16(output)\n",
        "        output = self.layer17(output)\n",
        "        output = self.layer18(output)\n",
        "\n",
        "      \n",
        "        #print(output.shape)\n",
        "        #output = F.avg_pool2d(output, [output.size(2), output.size(3)], stride=1)\n",
        "        output = self.avgpool(output)\n",
        "        #print(output.shape)\n",
        "        #output = output.reshape(output.shape[0], output.shape[1])\n",
        "        output = output.reshape(output.shape[0], -1)\n",
        "        #print(output.shape)\n",
        "        \n",
        "        output = self.classifier(output)\n",
        "        embedding = output\n",
        "        label_output = self.linear_label_last(output)\n",
        "        #label_output = self.linear_label(output)\n",
        "        #label_output = label_output/torch.norm(self.linear_label.weight, dim=1)\n",
        "        \n",
        "        # Create the feature embedding for the Center Loss\n",
        "        #closs_output = self.linear_closs(output)\n",
        "        #closs_output = self.relu_closs(closs_output)\n",
        "\n",
        "        #return closs_output, label_output\n",
        "        return label_output, embedding\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhUOJiEY0p2C"
      },
      "source": [
        "model = Network_new(3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learningRate = 0.005\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate, weight_decay=0.0005, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XJx9d2y0-nO"
      },
      "source": [
        "def train(model, data_loader, test_loader, task='Classification'):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(numEpochs):\n",
        "        avg_loss = 0.0\n",
        "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            #print(feats.device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(feats)[0]\n",
        "\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            avg_loss += loss.item()\n",
        "\n",
        "            if batch_num % 50 == 49:\n",
        "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
        "                avg_loss = 0.0    \n",
        "            \n",
        "            torch.cuda.empty_cache()\n",
        "            del feats\n",
        "            del labels\n",
        "            del loss\n",
        "        \n",
        "        if task == 'Classification':\n",
        "            val_loss, val_acc = test_classify(model, test_loader)\n",
        "            train_loss, train_acc = test_classify(model, data_loader)\n",
        "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "                  format(train_loss, train_acc, val_loss, val_acc))\n",
        "            #test_verify(model, threshold=0.4)\n",
        "            #test_verify_test(model, threshold=0.4, epoch=epoch)\n",
        "        scheduler.step()\n",
        "        #if task == 'Verification':\n",
        "         # test_verify(model, threshold=0.8, text=textloc)\n",
        "\n",
        "\n",
        "def test_classify(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "    accuracy = 0\n",
        "    total = 0\n",
        "    #with torch.no_grad():\n",
        "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "        outputs = model(feats)[0]\n",
        "        \n",
        "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
        "        pred_labels = pred_labels.view(-1)\n",
        "        \n",
        "        loss = criterion(outputs, labels.long())\n",
        "        \n",
        "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "        total += len(labels)\n",
        "        test_loss.extend([loss.item()]*feats.size()[0])\n",
        "        del feats\n",
        "        del labels\n",
        "\n",
        "    model.train()\n",
        "    return np.mean(test_loss), accuracy/total\n",
        "\n",
        "def testing(model, test_loader, criterion):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        total_predictions = 0.0\n",
        "        correct_predictions = 0.0\n",
        "        model.cuda()\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "            outputs,embeddings = model(data)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_predictions += target.size(0)\n",
        "            correct_predictions += (predicted == target).sum().item()\n",
        "\n",
        "            loss = criterion(outputs, target).detach()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        acc = (correct_predictions/total_predictions)*100.0\n",
        "        running_loss /= len(test_loader)\n",
        "        \n",
        "        print('Test Loss: ', running_loss)\n",
        "        print('Test Accuracy: ', acc, '%')\n",
        "        return running_loss, acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM7Dv6kT0-pr"
      },
      "source": [
        "n_epochs = 30\n",
        "training_loss = []\n",
        "testing_loss = []\n",
        "testing_acc = []\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    print(\"Epoch No.\",i+1)\n",
        "    train_loss = train(model, train_loader, criterion, optimizer)\n",
        "    test_loss, test_acc = testing(model, test_loader, criterion)\n",
        "    training_loss.append(train_loss)\n",
        "    testing_loss.append(test_loss)\n",
        "    testing_acc.append(test_acc)\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}